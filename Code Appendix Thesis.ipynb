{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module code (modules.py)\n",
    "## Basic module object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _module (object):    \n",
    "    \n",
    "    \"\"\"\n",
    "    A module can store feedback\n",
    "    \"\"\"  \n",
    "    def __init__(self):  \n",
    "        self._feedback = None\n",
    "    \n",
    "    \"\"\"\n",
    "    A most support some means of falsifying an input\n",
    "    \"\"\"  \n",
    "    def falsify(self, *any_input, verbose=False):\n",
    "        self._feedback = None\n",
    "    \n",
    "    \"\"\"\n",
    "    Simple way of retrieving stored feedback  \n",
    "    \"\"\"  \n",
    "    def getFeedback(self):\n",
    "        if self._feedback == None:\n",
    "            raise ValueError(\"Falsification Machine error: getFeedback() called before falsify()\")\n",
    "        return self._feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module 0a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class m0a (_module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        #self._message = \"Are you sure?\"\n",
    "        self._message = \"Weet je het zeker?\"\n",
    "    \n",
    "    def falsify(self, *any_input, verbose=False):\n",
    "        self._feedback = []\n",
    "        # The most basic of feedback, with a 50% (guessing) confidence\n",
    "        # tuple of (feedback, confidence, id, weight)\n",
    "        self._feedback.append( (self._message, 25, self._message, 1) )\n",
    "        if verbose:\n",
    "            return self._message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class m0b (_module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self._message = \"Does the case mention '\"\n",
    "        \n",
    "    def _lookup_symptoms(self, symptoms, df):\n",
    "                \n",
    "        symptom_list = pd.Series(0, index=df.columns)\n",
    "        for i, string, in enumerate(df.columns):\n",
    "            if i>0 and re.search(\"\\W\"+string+\"\\W\",symptoms):\n",
    "                symptom_list[string] = 1\n",
    " \n",
    "        return symptom_list\n",
    "    \n",
    "    def falsify(self, case, lookup, verbose=False):\n",
    "        self._feedback = []\n",
    "\n",
    "        try:\n",
    "            keywords = self._lookup_symptoms(case, lookup) #(dcs)\n",
    "        except:\n",
    "            return \"Falsification module 0b failed to interpret given case.\"\n",
    "        \n",
    "        for i in range(len(keywords)):\n",
    "            if i > 0 and keywords[i] != 0:\n",
    "                key = lookup.columns[i]\n",
    "                self._feedback.append( (self._message+key+\"'?\", 25, key, 1) )\n",
    "        if verbose:\n",
    "            return self._feedback[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class m1 (_module):\n",
    "    \n",
    "    def __init__(self, symmetrical=False, ab=True, ba=True):\n",
    "        super().__init__()\n",
    "        self._symmetrical = symmetrical\n",
    "        self._ab = ab\n",
    "        self._ba = ba\n",
    "        self._message = \"Did you consider '\"\n",
    "    \n",
    "    def _evaluate(self, a, b, sort=False):\n",
    "        tot_err, tot, inc = 0, 0, 0\n",
    "        errors = [] #(error size, index)\n",
    "        for i in range(1, len(a)):\n",
    "            \"\"\"\n",
    "            For all keys in a related to keys in b\n",
    "            \"\"\"\n",
    "            if self._symmetrical or a[i] != 0:\n",
    "                \"\"\"\n",
    "                Add the difference in weight (according to lookup table) to error rate\n",
    "                \"\"\"\n",
    "                err = abs(a[i] - b[i])\n",
    "                if (err > 0):\n",
    "                    errors.append( (1+ err/2, i) )\n",
    "                    tot_err += min(err, 1)\n",
    "                    #print(a[i], b[i], err, 1+abs(a[i]))\n",
    "                tot += 1\n",
    "\n",
    "        if sort:\n",
    "            errors = errors.sort()\n",
    "        if tot != 0:\n",
    "            inc = (1 -tot_err/tot )* 100\n",
    "\n",
    "        return inc, errors\n",
    "    \n",
    "    def _do_evaluate(self, a, b, df):\n",
    "        analysis = \"\"\n",
    "        \n",
    "        if self._symmetrical:\n",
    "            confidence, conf_errs = self._evaluate(a, b)\n",
    "            for error in conf_errs:\n",
    "                # tuple of (feedback, confidence, id, weight)\n",
    "                key = str(df.columns[error[1]])\n",
    "                self._feedback.append( (self._message+key+\"'?.\", confidence, key, error[0]) )\n",
    "                if conf_errs:\n",
    "                    analysis += \"- Your solution accounts for \"+str(round(confidence, 2)) +\"% of key aspects I evaluated.\\n\"\n",
    "                    conf_errs.sort()\n",
    "                    analysis += \"- Your solution fails to account for'\"+str(df.columns[conf_errs[-1][1]])+\"' (weight: \"+str(conf_errs[-1][0])+\").\\n\"\n",
    "        \n",
    "        if self._ab:\n",
    "            accounted, acc_errs = self._evaluate(a, b)\n",
    "            for error in acc_errs:\n",
    "                # tuple of (feedback, confidence, id, weight)\n",
    "                key = str(df.columns[error[1]])\n",
    "                self._feedback.append( (self._message+key+\"'?.\", accounted, key, error[0]) )\n",
    "                if acc_errs:\n",
    "                    analysis += \"- Your solution accounts for \"+str(round(accounted, 2)) +\"% of key aspects I evaluated.\\n\"\n",
    "                    acc_errs.sort()\n",
    "                    analysis += \"- Your solution fails to account for'\"+str(df.columns[acc_errs[-1][1]])+\"' (weight: \"+str(acc_errs[-1][0])+\").\\n\"\n",
    "        if self._ba:\n",
    "            incongruent, inc_errs = self._evaluate(b, a)\n",
    "            for error in inc_errs:\n",
    "                # tuple of (feedback, confidence, id, weight)\n",
    "                key = str(df.columns[error[1]])\n",
    "                self._feedback.append( (self._message+key+\"'?\", incongruent, key, error[0]) )\n",
    "                if inc_errs:\n",
    "                    analysis += \"- \"+str(round(incongruent, 2)) +\"% of key aspects I evaluated match with your solution.\\n\"\n",
    "                    inc_errs.sort()\n",
    "                    analysis += \"- Does the case mention '\"+str(df.columns[inc_errs[-1][1]])+\"' (weight: \"+str(inc_errs[-1][0])+\")?\\n\"\n",
    "                    \n",
    "        return analysis\n",
    "    \n",
    "    \"\"\"\n",
    "    This module uses: \n",
    "        - a support solution\n",
    "        - a proposed solution\n",
    "        - a lookup table\n",
    "    To produce a confidence level.\n",
    "    \"\"\"\n",
    "    def falsify(self, support, user, df, verbose=False):\n",
    "        self._feedback = []\n",
    "\n",
    "        try:\n",
    "            a = df.loc[df['diagnose'] == support].iloc[0]  #(dcs)\n",
    "        except:\n",
    "            return 'Falsification module 1 failed to interpret support solution.'\n",
    "\n",
    "        try:\n",
    "            b = df.loc[df['diagnose'] == user].iloc[0] #(e\n",
    "        except:\n",
    "            return 'Falsification module 1 failed to interpret user solution.'     \n",
    "\n",
    "        analysis = \"Module 1 says: Did you consider the following?\\n\"\n",
    "        analysis += self._do_evaluate(a, b, df)\n",
    "        \n",
    "        if verbose:\n",
    "            return analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module 2a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class m2a (m1):\n",
    "    \n",
    "    def __init__(self, symmetrical=False, ab=True, ba=True):\n",
    "        super().__init__(symmetrical, ab, ba)\n",
    "        self._message = \"Does the case mention '\"\n",
    "    \n",
    "    def _lookup_symptoms(self, symptoms, df):\n",
    "                \n",
    "        symptom_list = pd.Series(0, index=df.columns)\n",
    "        for i, string, in enumerate(df.columns):\n",
    "            if i>0 and re.search(\"\\W\"+string+\"\\W\",symptoms):\n",
    "                symptom_list[string] = 1\n",
    " \n",
    "        return symptom_list\n",
    "    \n",
    "    \"\"\"\n",
    "    This module uses: \n",
    "        - a case (problem description)\n",
    "        - a proposed solution\n",
    "        - a lookup table\n",
    "    To produce a confidence level.\n",
    "    \"\"\"\n",
    "    def falsify(self, symptoms, user, df, verbose=False):\n",
    "        self._feedback = []\n",
    "\n",
    "        try:\n",
    "            a = self._lookup_symptoms(symptoms, df) #(dcs)\n",
    "        except:\n",
    "            return \"Falsification module 2 failed to interpret given case.\"\n",
    "        \n",
    "        try:\n",
    "            b = df.loc[df['diagnose'] == user].iloc[0]\n",
    "        except:\n",
    "            return 'Falsification module 2 failed to interpret given solution.'  \n",
    "\n",
    "        analysis = \"Module 2 says: Did you consider the following?\\n\"\n",
    "        analysis += self._do_evaluate(a, b, df)\n",
    "        \n",
    "        if verbose:\n",
    "            return analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module 2b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class m2b (m2a):\n",
    "    \n",
    "    def _lookup_symptoms(self, symptoms, df):\n",
    "                \n",
    "        symptom_list = pd.Series(0, index=df.columns)\n",
    "        for i, string, in enumerate(df.columns):\n",
    "            if i>0 and re.search(\"\\W\"+string+\"\\W\",symptoms):\n",
    "                if re.search(\"( erg | vaak | hevig | veel | sterk | ernstig | flink ){1}[ \\w]*\"+string+\"{1}\", symptoms) or re.search(string+\"{1}\"+\"[ \\w]*( erg| vaak| hevig| veel| sterk| ernstig| flink)\", symptoms):\n",
    "                    #print('- veel '+string)\n",
    "                    symptom_list[string] = 1\n",
    "                else:\n",
    "                    #print('- '+string)\n",
    "                    symptom_list[string] = 0.75\n",
    "                if re.search(\"( niet | geen | nauwelijks | nooit | weinig ){1}[ \\w]*\"+string+\"{1}\", symptoms) or re.search(string+\"{1}\"+\"[ \\w]*( niet| geen| nauwelijks| nooit| weinig){1}\", symptoms):\n",
    "                    #print('- niet '+string)\n",
    "                    symptom_list[string] = -symptom_list[string]\n",
    "\n",
    "        return symptom_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class m3 (m1):\n",
    "    def falsify(self, generated, user, df, verbose=False):\n",
    "        self._feedback = []\n",
    "\n",
    "        try:\n",
    "            a = df.loc[df['diagnose'] == generated].iloc[0]  #(dcs)\n",
    "        except:\n",
    "            return \"Falsification module 1 failed to interpret 'generated' solution.\"\n",
    "\n",
    "        try:\n",
    "            b = df.loc[df['diagnose'] == user].iloc[0] #(e\n",
    "        except:\n",
    "            return 'Falsification module 1 failed to interpret user solution.'     \n",
    "\n",
    "        analysis = \"Module 3 says: Did you consider the following?\\n\"\n",
    "        analysis += self._do_evaluate(a, b, df)\n",
    "        \n",
    "        if verbose:\n",
    "            return analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregator code (aggregators.py)\n",
    "## Aggregator v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class v1 (object):\n",
    "    \n",
    "    \"\"\"\n",
    "    Takes any number of modules and aggregates their feedback for the user\n",
    "    \"\"\"\n",
    "    def aggregate(self, *modules, debug=False):\n",
    "        critiques = []\n",
    "        \n",
    "        for module in modules:\n",
    "            for feedback in module.getFeedback():\n",
    "                if debug:\n",
    "                    critiques.append( (feedback[1] / feedback[3], feedback[2]) )\n",
    "                else:\n",
    "                    critiques.append( (feedback[1] / feedback[3], feedback[0]) )\n",
    "        critiques.sort()\n",
    "        \n",
    "        if debug:\n",
    "            return critiques\n",
    "        return critiques[0][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregator v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class v2 (object):\n",
    "    \n",
    "    \"\"\"\n",
    "    Takes any number of modules and aggregates their feedback for the user\n",
    "    \"\"\"\n",
    "    def aggregate(self, *modules, debug=False):\n",
    "        \n",
    "        critiques = []\n",
    "        \n",
    "        for module in modules:\n",
    "            for feedback in module.getFeedback():\n",
    "                critiques.append( (feedback[2], feedback[0], feedback[1], feedback[3]) )\n",
    "        critiques.sort()\n",
    "        \n",
    "        i = 0\n",
    "        n = len(critiques)\n",
    "        reduced = []\n",
    "        while i<n:\n",
    "            x = 1\n",
    "            tot = ( critiques[i][2] / critiques[i][3] )\n",
    "            while i+1<n and critiques[i][0] == critiques[i+1][0]:\n",
    "                tot += ( critiques[i+1][2] / critiques[i+1][3] )\n",
    "                i += 1\n",
    "                x += 1\n",
    "            confidence = tot/x\n",
    "            if confidence > 0:  \n",
    "                if debug:\n",
    "                    reduced.append( (confidence, critiques[i][0]) )\n",
    "                else:\n",
    "                    reduced.append( (confidence, critiques[i][1]) )\n",
    "            i += 1\n",
    "        reduced.sort()\n",
    "        if debug:\n",
    "            return reduced\n",
    "        return reduced[0][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregator v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class v3 (object):\n",
    "    \n",
    "    \"\"\"\n",
    "    Takes any number of modules and aggregates their feedback for the user\n",
    "    \"\"\"\n",
    "    def aggregate(self, *modules, debug=False):\n",
    "        \n",
    "        critiques = []\n",
    "        \n",
    "        for module in modules:\n",
    "            for feedback in module.getFeedback():\n",
    "                critiques.append( (feedback[2], feedback[0], feedback[1], feedback[3]) )\n",
    "        critiques.sort()\n",
    "        \n",
    "        i = 0\n",
    "        n = len(critiques)\n",
    "        reduced = []\n",
    "        while i<n:\n",
    "            x = 1\n",
    "            tot = ( critiques[i][2], critiques[i][3] )\n",
    "            while i+1<n and critiques[i][0] == critiques[i+1][0]:\n",
    "                tot += ( critiques[i+1][2], critiques[i+1][3] )\n",
    "                i += 1\n",
    "                x += 1\n",
    "            confidence = (tot[0]) / (tot[1])\n",
    "            if confidence > 0:  \n",
    "                if debug:\n",
    "                    reduced.append( (confidence, critiques[i][0]) )\n",
    "                else:\n",
    "                    reduced.append( (confidence, critiques[i][1]) )\n",
    "            i += 1\n",
    "        reduced.sort()\n",
    "        if debug:\n",
    "            return reduced\n",
    "        return reduced[0][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregator v4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class v4 (object):\n",
    "    \n",
    "    \"\"\"\n",
    "    Takes any number of modules and aggregates their feedback for the user\n",
    "    \"\"\"\n",
    "    def aggregate(self, *modules, debug=False):\n",
    "        \n",
    "        sub_reduced = []\n",
    "        for module in modules:\n",
    "            critiques = []\n",
    "            for feedback in module.getFeedback():\n",
    "                critiques.append( (feedback[2], feedback[0], feedback[1], feedback[3]) )\n",
    "            critiques.sort()\n",
    "            \n",
    "            i = 0\n",
    "            n = len(critiques)\n",
    "            while i<n:\n",
    "                x = 1\n",
    "                tot = ( critiques[i][2], critiques[i][3] )\n",
    "                while i+1<n and critiques[i][0] == critiques[i+1][0]:\n",
    "                    tot += ( critiques[i+1][2], critiques[i+1][3] )\n",
    "                    i += 1\n",
    "                    x += 1\n",
    "                sub_reduced.append( (critiques[i][0], critiques[i][1], tot[0]/tot[1]) )\n",
    "                i += 1\n",
    "        sub_reduced.sort()\n",
    "        #print(sub_reduced)\n",
    "\n",
    "        reduced = []\n",
    "        i = 0\n",
    "        n = len(sub_reduced)\n",
    "        while i<n:\n",
    "            x = 1\n",
    "            tot = ( sub_reduced[i][2] )\n",
    "            while i+1<n and sub_reduced[i][0] == sub_reduced[i+1][0]:\n",
    "                tot += ( sub_reduced[i+1][2] )\n",
    "                i += 1\n",
    "                x += 1\n",
    "            confidence = (tot)/x\n",
    "            if confidence > 0:\n",
    "                if debug:\n",
    "                    reduced.append( (confidence, sub_reduced[i][0]) )\n",
    "                else:\n",
    "                    reduced.append( (confidence, sub_reduced[i][1]) )\n",
    "            i += 1\n",
    "        reduced.sort()\n",
    "        if debug:\n",
    "            return reduced\n",
    "        print(reduced)\n",
    "        return reduced[0][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregator v5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class v5 (object):\n",
    "    \n",
    "    \"\"\"\n",
    "    Takes any number of modules and aggregates their feedback for the user\n",
    "    \"\"\"\n",
    "    def aggregate(self, *modules, debug=False):\n",
    "        \n",
    "        sub_reduced = []\n",
    "        for module in modules:\n",
    "            critiques = []\n",
    "            for feedback in module.getFeedback():\n",
    "                critiques.append( (feedback[2], feedback[0], feedback[1], feedback[3]) )\n",
    "            critiques.sort()\n",
    "            \n",
    "            i = 0\n",
    "            n = len(critiques)\n",
    "            while i<n:\n",
    "                x = 1\n",
    "                tot = ( critiques[i][2], critiques[i][3] )\n",
    "                while i+1<n and critiques[i][0] == critiques[i+1][0]:\n",
    "                    tot += ( critiques[i+1][2], critiques[i+1][3] )\n",
    "                    i += 1\n",
    "                    x += 1\n",
    "                sub_reduced.append( (critiques[i][0], critiques[i][1], tot[0]/x, tot[1]/x) )\n",
    "                i += 1\n",
    "        sub_reduced.sort()\n",
    "        #print(sub_reduced)\n",
    "\n",
    "        reduced = []\n",
    "        i = 0\n",
    "        n = len(sub_reduced)\n",
    "        while i<n:\n",
    "            x = 1\n",
    "            tot = ( sub_reduced[i][2], sub_reduced[i][3] )\n",
    "            while i+1<n and sub_reduced[i][0] == sub_reduced[i+1][0]:\n",
    "                tot += ( sub_reduced[i+1][2], sub_reduced[i+1][3] )\n",
    "                i += 1\n",
    "                x += 1\n",
    "            confidence = tot[0]/tot[1]\n",
    "            if confidence > 0:\n",
    "                if debug:\n",
    "                    reduced.append( (confidence, sub_reduced[i][0]) )\n",
    "                else:\n",
    "                    reduced.append( (confidence, sub_reduced[i][1]) )\n",
    "            i += 1\n",
    "        reduced.sort()\n",
    "        if debug:\n",
    "            return reduced\n",
    "        print(reduced)\n",
    "        return reduced[0][1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
